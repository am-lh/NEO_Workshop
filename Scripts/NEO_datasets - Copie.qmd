---
title: "NEO Workshop"
author: "Am√©lie Lehuen"
description: "add info on script"
date: "`r format(Sys.time(), '%d %B %Y')`"
editor: visual
execute:
  eval: true
  echo: false
  message: false
  warning: false
  output: true
  include: false
editor_options: 
  chunk_output_type: console
---

# Introduction

The aim of the workshop is to...

## Script preparation

```{r}
#| label: load-packages
#| code-summary: "Packages"

library(knitr)
library(readxl) ; library(openxlsx)
library(beepr); library(clipr)
library(tidyverse); library(data.table)
library(broom); library(purrr) # tidy,glance,augment
library(lubridate)
library(rstatix); library(Hmisc)  # corr and pvalue calculation

# SIG
library(sf); library(sfheaders)
library(rnaturalearth) # library(raster)
library(tmap)
library(tmaptools) # tmap_mode; for static and interactive maps

# Graphics packages
library(scales); library(grafify)
library(RColorBrewer)
library(ggpubr)
library(GGally)
library(plotly)
```

```{r}
#| label: workenvir

rm(list=ls())

wdsource <- "Data/"
wdres <- "Results/"
wdmat <- "Matrices/"
wdgraph <- "Plots/"
wdGIS <- "SIG/"

refresh_SNO<-FALSE # TRUE to collect data online and recreate matrices
```

```{r}
#| label: graphchart

theme_set(theme_bw(base_size = 16)) # theme_gray() theme_bw() theme_light() theme_dark()
# My_Theme <- theme(
#   axis.title.x = element_text(size = 16),
#   axis.title.y = element_text(size = 16))

pal_clb <- function(x) {graf_col_palette(palette = "muted")(x)}; # show_col(pal_clb(4))
colbdd <- pal_clb(4)
colBin<-c(pal_clb(6)[1],pal_clb(6)[2]); colMisc<-pal_clb(6)[6]# show_col(colBin)
Scalc_bin<- function() {scale_colour_manual(values=colBin)}
Scalc_lon<- function(x) {scale_colour_manual(values=pal_clb(x))}
# Scale_brew <- function() {scale_colour_brewer(palette="Spectral",aesthetics=c("colour","fill"))}
```

```{r}
#| label: functmade

loadRData <- function(fileName){
#loads an RData file, and returns it
    load(fileName)
    mget(ls()[ls() != "fileName"])
}

# ggpairs custom colors on corr
corr_col <- function(data, mapping, method="p", use="pairwise", ...){
  # grab data
  x <- eval_data_col(data, mapping$x)
  y <- eval_data_col(data, mapping$y)
  # calculate correlation
  corr <- cor(x, y, method=method, use=use)
  # calculate colour based on correlation value
  colFn <- colorRampPalette(c("blue", "white", "red"), interpolate ='spline')
  fill <- colFn(100)[findInterval(corr, seq(-1, 1, length=100))]
  ggally_text(
    label = as.character(round(corr, 2)),
    mapping = aes(),
    xP = 0.5, yP = 0.5,
    ...) + #  ggally_cor(data = data, mapping = mapping, ...) + 
    theme_void() +
    theme(panel.background = element_rect(fill=fill))

} #wrap(cor_func,method = 'spearman', symbol = "Corr:\n")

# Function for corr ad pvalue table x is a matrix containing the data
# method : correlation method. "pearson"" or "spearman"" is supported
# removeTriangle : remove upper or lower triangle
# results :  if "html" or "latex"
# the results will be displayed in html or latex format
corstars <-function(x, method=c("pearson", "spearman"), 
                    removeTriangle=c("upper", "lower"),
                     result=c("none", "html", "latex")){
    #Compute correlation matrix
    require(Hmisc)
    x <- as.matrix(x)
    correlation_matrix<-rcorr(x, type=method[1])
    R <- correlation_matrix$r # Matrix of correlation coeficients
    p <- correlation_matrix$P # Matrix of p-value 
    
    ## Define notions for significance levels; spacing is important.
    mystars <- ifelse(p < .0001, "****", 
                      ifelse(p < .001, "***", 
                             ifelse(p < .01, "**", 
                                    ifelse(p < .05, "*", " "))))
    
    ## trunctuate the correlation matrix to two decimal
    R <- format(round(cbind(rep(-1.11, ncol(x)), R), 2))[,-1]
    
    ## build a new matrix that includes the correlations with their apropriate stars
    Rnew <- matrix(paste(R, mystars, sep=""), ncol=ncol(x))
    diag(Rnew) <- paste(diag(R), " ", sep="")
    rownames(Rnew) <- colnames(x)
    colnames(Rnew) <- paste(colnames(x), "", sep="")
    
    ## remove upper/lower triangle of correlation matrix
    if(removeTriangle[1]=="upper"){
      Rnew <- as.matrix(Rnew)
      Rnew[upper.tri(Rnew, diag = TRUE)] <- ""
    }
    else if(removeTriangle[1]=="lower"){
      Rnew <- as.matrix(Rnew)
      Rnew[lower.tri(Rnew, diag = TRUE)] <- ""
    }
    
    ## remove last column and return the correlation matrix
    Rnew <- as.data.frame(Rnew)
    Rnew <- cbind(Rnew[1:length(Rnew)-1])
    if (result[1]=="none") return(Rnew)
    else{
      if(result[1]=="html") print(xtable(Rnew), type="html")
      else print(xtable(Rnew), type="latex") 
    }
} 

```

# Load of Data

## Basic variables

```{r}
#| label: basicvar

name_bases <- c("BENTHOBS","PHYTOBS","SOMMLIT","COASTHF")

SNO_Areas<-paste(wdGIS,"SNO_Areas.shp",sep="")
  SNO_Areas<-st_read(SNO_Areas,quiet=TRUE,crs=4326 ) # %>% st_transform(2154)
# st_crs(SNO_Areas)
```

## Output binder and data

```{r}
#| label: outfiles

binderout <- sprintf("%sNEO_wshp.xlsx",wdres)
rdataout  <- sprintf("%sNEO_wshp",wdmat)
# if exists
# load(rdataout)
```

## BENTHOBS

Data are available on https://data.benthobs.fr/. There are different files: - **granulometry** TSV file with granulometry data. - **hydrology** TSV file with hydrology data. - **macrofauna** TSV file with macrofauna data. - **organicmatter** TSV file with organic matter data

```{r}
#| label: benthos_load

if (refresh_SNO){
  benthobs_site <- "https://data.benthobs.fr/files/latest/all_sites/"
  download.file(paste0(benthobs_site,"granulometry.tsv"),
                          destfile=sprintf("%sBENTHOBS/granulometry.tsv",wdsource))
  # download.file(paste0(benthobs_site,"hydrology.tsv"),
  #                         destfile=sprintf("%sBENTHOBS/hydrology.tsv",wdsource))
  download.file(paste0(benthobs_site,"macrofauna.tsv"),
                          destfile=sprintf("%sBENTHOBS/macrofauna.tsv",wdsource))
  download.file(paste0(benthobs_site,"organicmatter.tsv"),
                          destfile=sprintf("%sBENTHOBS/organicmatter.tsv",wdsource))
  name_base <- "BENTHOBS"
  name_file<-c("granulometry","macrofauna","organicmatter") #,"hydrology"
  name_df<-c("bo_granu","bo_macro","bo_orga") #,"bo_hydro"
  
  bo_set_raw <- lapply(sprintf("%sBENTHOBS/%s.tsv",
            wdsource,name_file),
            read_tsv)
  
  bo_set <- bo_set_raw %>%
    setNames(name_df) %>% 
    Map(cbind, ., bdd_table=names(.)) %>% 
    lapply(.  %>%
      as.data.frame() %>% 
      rename(sampling_date=`Sampling date`,
             station=Station,
             longitude=Longitude,
             latitude=Latitude) %>% 
      mutate(bdd_base = name_base) %>%
      mutate(dfMonth = month.abb[month(sampling_date)],
             dfYear  = year(sampling_date)) %>%
      relocate(bdd_base,bdd_table) %>% 
      mutate(across(where(is.logical),as.character)) %>% 
      select(-contains("...")))
  
  bo_set <- bo_set %>%
    lapply(.  %>%
     # slight change of title of stations to distinguish them from other sno
      mutate(station=ifelse(station=="Comprian", "Comprian_bo", station)) %>%
      mutate(station=ifelse(station=="Antioche", "Antioche_bo", station)))
  
  bo_setvar <- Reduce(intersect, lapply(bo_set, names))
  
  bo_sf <- lapply(bo_set, st_as_sf, 
                  coords=c("longitude","latitude"),
                  crs=4326,remove = FALSE)
  bo_coord <- bo_sf %>% lapply( . %>% 
    select(c("bdd_base","bdd_table","station","longitude","latitude")) %>% 
    summarise(base::unique(.))) %>% 
    bind_rows() %>% 
    as.data.frame(.) %>% 
    mutate("TRUE" = TRUE) %>% 
    pivot_wider(names_from = bdd_table, values_from = "TRUE", values_fill = FALSE) %>% 
    st_as_sf()
  
  bo_set <- bo_set %>%
    lapply(.  %>%
      mutate(across(c(station,dfMonth,dfYear), as.factor)))
  
  save(list=(ls(pattern="bo*")),file = sprintf("%s_bo.RData",rdataout))
} else {
  load(file = sprintf("%s_bo.RData",rdataout))
}
```

## PHYTOBS

Data are available on https://data.phytobs.fr/. There are different files: - **Analyst** files containing single taxon counts. - **Phytobs** files containing single counts for taxon groups that are part of the SNO labelled taxon groups. - **combined** files aggregating the two previous tables?

```{r}
#| label: phythos_load

if (refresh_SNO){
  phytobs_site <- "https://data.phytobs.fr/files/latest/all_sites/"
  download.file(paste0(phytobs_site,"Phytobs.csv"),
                          destfile="./Data/PHYTOBS/Phytobs.csv")
  download.file(paste0(phytobs_site,"Analyst.csv"),
                          destfile="./Data/PHYTOBS/Analyst.csv")
  download.file(paste0(phytobs_site,"combined.csv"),
                          destfile="./Data/PHYTOBS/combined.csv")
  name_base <- "PHYTOBS"
  name_file<-c("Analyst","Phytobs","combined")
  name_df<-c("po_analy","po_phyto","po_comb")
  
  po_set_raw <- lapply(sprintf("%sPHYTOBS/%s.csv",
            wdsource,name_file),
            read_delim,delim=";")
  
  po_set <- po_set_raw %>%
    setNames(name_df) %>% 
    Map(cbind, ., bdd_table=names(.)) %>% 
    lapply(.  %>%
      as.data.frame() %>% 
      rename(sampling_date=sampling_date,
             station=site,
             longitude=longitude,
             latitude=latitude) %>% 
      mutate(bdd_base = name_base) %>%
      mutate(dfMonth = month.abb[month(sampling_date)],
             dfYear  = year(sampling_date)) %>%
      mutate(across(where(is.logical),as.character)) %>% 
      select(-contains("...")) %>% 
      relocate(bdd_base,bdd_table))
  
  po_set <- po_set %>%
    lapply(.  %>%
     # slight change of title of stations to distinguish them from other sno
      mutate(station=ifelse(station=="Luc-sur-Mer", "Luc-sur-Mer_po", station)))
  
  po_setvar <- Reduce(intersect, lapply(po_set, names))
  
  po_sf <- lapply(po_set,st_as_sf, coords=c("longitude","latitude"),
                      crs=4326,remove = FALSE)
  po_coord <- po_sf %>% lapply( . %>% 
    select(c("bdd_base","bdd_table","station","longitude","latitude")) %>% 
    summarise(unique(.))) %>% 
    bind_rows() %>% 
    as.data.frame(.) %>% 
    mutate("TRUE" = TRUE) %>% 
    pivot_wider(names_from = bdd_table, values_from = "TRUE", values_fill = FALSE) %>% 
    st_as_sf()
  
  po_set <- po_set %>%
    lapply(.  %>%
      mutate(across(c(station,dfMonth,dfYear), as.factor)))
  
  save(list=(ls(pattern="po*")),file = sprintf("%s_po.RData",rdataout))
} else {
  load(file = sprintf("%s_po.RData",rdataout))
}

```

## SOMLIT

Data are available on https://www.somlit.fr/demande-de-donnees/. You have to request with your mail each files available. Please refer to [@li√©nart2017], [@li√©nart2018], [@cocquempot2019] and [@lheureux2022] for detail about the dataset building and history. Parameters available are in @fig-parsom. ![](../images/tableau_parametre.png){#fig-parsom}

```{r}
#| label: somlit_load

if (refresh_SNO){
  name_base <- "SOMLIT"
  name_file<-c("Somlit_Extraction_ctd",
               "Somlit_Extraction_hydro",
               "Somlit_Extraction_piconano")
  name_df<-c("sl_ctd","sl_hydro","sl_piconano")
  
  sl_set_raw <- lapply(sprintf("%sSOMLIT/%s.csv",
            wdsource,name_file),
            read_delim,skip = 2,comment="//",delim=";")
  
  sl_set <- sl_set_raw %>%
    setNames(name_df) %>% 
    Map(cbind, ., bdd_table=names(.)) %>% 
    lapply(.  %>%
      as.data.frame() %>% 
      setNames(sub("\\*", "", names(.))) %>% 
      rename(sampling_date=DATE,
             station=nomSite,
             longitude=gpsLong,
             latitude=gpsLat) %>% 
      mutate(bdd_base = name_base) %>%
      mutate(dfMonth = month.abb[month(sampling_date)],
             dfYear  = year(sampling_date)) %>%
      mutate(across(where(is.logical),as.character)) %>% 
      select(-contains("...")) %>% 
      relocate(bdd_base,bdd_table))
  
  sl_setvar <- Reduce(intersect, lapply(sl_set, names))
  
  sl_sf <- lapply(sl_set,st_as_sf, coords=c("longitude","latitude"),
                      crs=4326,remove = FALSE)
  sl_coord <- sl_sf %>% lapply( . %>% 
    select(c("bdd_base","bdd_table","station","longitude","latitude")) %>% 
    summarise(unique(.))) %>% 
    bind_rows() %>% 
    as.data.frame(.) %>% 
    mutate("TRUE" = TRUE) %>% 
    pivot_wider(names_from = bdd_table, values_from = "TRUE", values_fill = FALSE) %>% 
    st_as_sf()
  
  sl_set <- sl_set %>%
    lapply(.  %>%
      mutate(across(c(station,dfMonth,dfYear), as.factor)))
  
  save(list=(ls(pattern="sl*")),file = sprintf("%s_sl.RData",rdataout))
} else {
  load(file = sprintf("%s_sl.RData",rdataout))
}

```

## COASTHF

Data are available on https://data.coriolis-cotier.org/fr. In the menu, the active platform toggle button is activated and the COASTHF network is selected. All available stations has been selected. Detailed information are available on https://coast-hf.fr/. The selected buoys in data are listed in @tbl-coashf

```{r}
#| label: coasthf_load

if (refresh_SNO){
  name_base <- "COASTHF"
  
  cf_buoy<-as.data.frame(rbind(
                 c("EXIN0003","POEM"),
                 c("EXIN0004","SOLEMIO"),
                 c("EXIN0002","EOL"),
                 c("EXIN0001","ARCACHON B13"),
                 c("6100284","Mesurho"),
                 c("EXIN0006","SOLA"),
                 c("6200021","Vilaine Molit"),
                 c("IF000700","SMART"),
                 c("6200450","Iroise Stanne"),
                 c("IF000997","Iroise Stanne TEST"),
                 c("6200310","Smile LucSurMer"),
                 c("SCENES","SCENES"),
                 c("6200443","Carnot"),
                 c("EXIN0005","ASTAN"))
                 )
  names(cf_buoy) <- c("Code","Name")
  
  cf_set_raw <- lapply(sprintf("%s/COASTHF/mooring-buoys-time-series-%s.csv",
            wdsource,cf_buoy[,1]),
            read_csv)
  
  cf_set <- cf_set_raw %>% 
    setNames(cf_buoy[,1]) %>% 
    Map(cbind, ., station=cf_buoy[,2]) %>%
    lapply(.  %>% 
      as.data.frame() %>% 
      mutate(PLATFORM = as.character(PLATFORM)) %>% 
      rename(sampling_date=`DATE (yyyy-mm-ddThh:mi:ssZ)`,
             bdd_table=PLATFORM,
             longitude=`LONGITUDE (degree_east)`,
             latitude=`LATITUDE (degree_north)`) %>% 
      mutate(bdd_base = name_base) %>%
      mutate(dfMonth = month.abb[month(sampling_date)],
             dfYear  = year(sampling_date)) %>%
      mutate(across(where(is.logical),as.character)) %>% 
      select(-contains("...")) %>%
      relocate(bdd_base,bdd_table))
  
  cf_setvar <- Reduce(intersect, lapply(cf_set, names))
  
  cf_sf <- lapply(cf_set,st_as_sf, coords=c("longitude","latitude"),
                      crs=4326,remove = FALSE)
  cf_coord <- cf_sf %>% lapply( . %>% 
    select(c("bdd_base","bdd_table","station","longitude","latitude")) %>% 
    summarise(unique(.))) %>% 
    bind_rows()
  
  cf_set <- cf_set %>%
    lapply(.  %>%
      mutate(across(c(station,dfMonth,dfYear), as.factor)))
  
  save(list=(ls(pattern="cf*")),file = sprintf("%s_cf.RData",rdataout))
} else {
  load(file = sprintf("%s_cf.RData",rdataout))
}

```

```{r}
#| label: tbl-coasthf
#| include: true
#| fig-cap: "List of buoys"

kable(cf_buoy)
```

# Global data sets description

All data treatment has been conducted with `r R.version.string` <!--# except for MARS3D pre-treatment on Matlab 2019a -->. Significance levels are tagged for p \< .0001 with "\*\*\*\*", p \< .001 with "\*\*\*", p \< .01 with "\*\*", p \< .05 with "\*".

## Map of sites

```{r}
#| label: prefig-map

sno_coord <- rbind(bo_coord %>%
                     select(c("bdd_base","station",
                              "longitude","latitude")),
                   po_coord %>%
                     select(c("bdd_base","station",
                              "longitude","latitude")),
                   sl_coord %>%
                     select(c("bdd_base","station",
                              "longitude","latitude")),
                   cf_coord %>% select(c("bdd_base","station","longitude","latitude")))
st_write(obj = sno_coord, paste(wdGIS,"sno_coord.shp",sep=""), delete_layer = TRUE)

SNO_Areas_S_tmp <- SNO_Areas %>% filter(Category %in% "SMALL")
SNO_Areas_M_tmp <- SNO_Areas %>% filter(Category %in% "MEDIUM")
SNO_Areas_L_tmp <- SNO_Areas %>% filter(Category %in% "LARGE")

sf_use_s2(FALSE)
sno_coord <- sno_coord %>%
  mutate(intersection = as.integer(st_intersects(geometry,SNO_Areas_S_tmp)),
                      Int_Area_Small = if_else(is.na(intersection),"NA",
                                               SNO_Areas_S_tmp$Int_Area[intersection])) %>%
  mutate(intersection = as.integer(st_intersects(geometry,SNO_Areas_M_tmp)),
                      Int_Area_Medium = if_else(is.na(intersection),"NA",
                                               SNO_Areas_M_tmp$Int_Area[intersection])) %>% 
  mutate(intersection = as.integer(st_intersects(geometry,SNO_Areas_L_tmp)),
                      Int_Area_Large = if_else(is.na(intersection),"NA",
                                               SNO_Areas_L_tmp$Int_Area[intersection])) %>% 
  select(-intersection)
sno_coord$Int_Area_Small[sno_coord$Int_Area_Small=="NA"]<-NA
sno_coord$Int_Area_Medium[sno_coord$Int_Area_Medium=="NA"]<-NA
sno_coord$Int_Area_Large[sno_coord$Int_Area_Large=="NA"]<-NA

bo_set <- bo_set %>% map(.,left_join,sno_coord, 
                         by = c("bdd_base", "station", 
                                "latitude", "longitude"))
po_set <- po_set %>% map(.,left_join,sno_coord, 
                         by = c("bdd_base", "station", 
                                "latitude", "longitude"))
sl_set <- sl_set %>% map(.,left_join,sno_coord, 
                         by = c("bdd_base", "station", 
                                "latitude", "longitude"))
cf_set <- cf_set %>% map(.,left_join,sno_coord, 
                         by = c("bdd_base", "station", 
                                "latitude", "longitude"))
```

```{r}
#| label: fig-map
#| include: true
#| fig-cap: "Maps of data sets locations"

tmap_mode("view") # "plot" "view"
tm_data<-
  tm_basemap(leaflet::providers$OpenStreetMap.HOT) +
  tm_shape(SNO_Areas) +
    tm_fill(col="Int_Area", alpha=0.5, popup.vars = TRUE,
            legend.show = FALSE) +
  tm_shape(sno_coord) +
    tm_dots(jitter = 0.01,
            col="bdd_base", palette=colbdd,size=0.1,
            clustering = FALSE, popup.vars = TRUE) +
  tm_layout(legend.outside = TRUE) #

tm_data +
  tm_minimap()
# tmap_save(tm_data, filename = "NEO_map.html")

```

```{r}
#| eval: false

# # base of bounding boxes
# boxMP<-c(xmin=-0.1, ymin=49.3, xmax=0.45, ymax=49.65)
# boxchannel<-c(xmin=-2.5, ymin=49.0, xmax=1.5, ymax=51.0)
# bay <- ne_states(country = c("France","united kingdom"), 
#                  returnclass = "sf") %>%
#   st_crop(boxchannel)
# bbbay<-bb(bay)
# # boxestuary<-bb(boxchannel)
# tm_bay<-
#   tm_shape(bay) + 
#     tm_fill(col="grey30") +
#   tm_shape(st_as_sfc(bbbay)) + 
#     tm_borders("red", lwd = 2) +
#   tm_layout(bg.color = "skyblue")
# 
# tmap_mode("plot") # "plot" "view"
# osm_df <- read_osm(bay, ext=1.1)
# tm_data<-
#   # tm_shape(estuary, bbox = boxestuary) +
#   #   tm_polygons() +
#   tm_shape(osm_df) +
#     tm_rgb() +
# tm_add_legend("symbol",
#               labels = name_bases,
#               col=colbdd) +
#   tm_scale_bar(position = c("LEFT", "BOTTOM"), width = 0.15) + #SCALE
#   tm_compass(position = c("RIGHT", "BOTTOM"), size = 3) +          #NORTH COMPASS
#   tm_layout(legend.position= c("LEFT","TOP"))
# vp <- grid::viewport(0.985, 0.995, width = 0.25, just=c("right", "top"))
# print(tm_bay, vp = vp)
# # tmap_save(tm_data,filename="../Graphiques/Estuary_map_with_inset.png",
# #           insets_tm=tm_bay, insets_vp=vp,
# #           dpi=600,
# #           height=10, units="cm")
```

## Datasets description

Main information about data sets are summed up in @tbl-desc_data

```{r}
#| label: tbl-desc_data
#| tbl-cap: "Summaries of all datatables from data sets"
#| include: true

sno_set<-c(bo_set,po_set,sl_set,cf_set) 
sno_setvar<-c(bo_setvar,po_setvar,sl_setvar,cf_setvar) %>%
  paste(collapse = ", ")
sno_names<-names(sno_set)
sno_all_desc<-list()
for (dfi in 1:length(sno_set)) {
  df<-sno_set[[dfi]]
  bdd_base<-unique(df$bdd_base)
  sno_name<-unique(df$bdd_table)
  obsnbtot <-count(df) %>%
    pull() %>% 
    formatC(., format="f", big.mark=",", digits=0)
  varnbtot <- ncol(df)
  yearsrange <- df %>% 
    summarise(across(sampling_date,
                     list(start=~year(min(.,na.rm=TRUE)),
                          end=~year(max(.,na.rm=TRUE))))) %>%
    unlist(., use.names=FALSE) %>%
    paste(collapse = " to ")
  nbstatmt <- df %>% 
    mutate(dfMonth=month.abb[month(sampling_date)]) %>%
    group_by(dfMonth) %>% 
    summarise(nbstat=n_distinct(station)) %>%
    arrange(desc(nbstat)) %>% 
    mutate(cumprop = cumsum(nbstat)/sum(nbstat)) %>% 
    filter(cumprop<=0.6) %>%
    arrange(match(dfMonth, month.abb)) %>% 
    pull(dfMonth,unique(cumprop)) %>% 
    paste(collapse = ", ")
  nbstattot <- df %>% 
    summarise(n_distinct(station)) %>% 
    pull()
  nbstatyr <- df %>% 
      mutate(dfYear  = year(sampling_date)) %>%
    group_by(dfYear) %>% 
    summarise(nbstat=n_distinct(station)) %>%
    summarise(nbstat=round(mean(nbstat),0)) %>% 
    pull()
  
  sno_all_desc[[dfi]]<-list(bdd_base =bdd_base,
                       sno_name=sno_name,
                       obsnbtot=obsnbtot,
                       varnbtot=varnbtot,
                       yearsrange=yearsrange,
                       nbstatmt=nbstatmt,
                       nbstattot=nbstattot,
                       nbstatyr=nbstatyr)
}

# sno_desc_tb <- rbindlist(sno_desc, idcol = TRUE)
sno_all_desc <- bind_rows(sno_all_desc) 
sno_all_desc %>% kable()

timevision <- sno_set %>% lapply(. %>% 
  select(bdd_base,bdd_table,
         sampling_date,station) %>% 
  mutate(sampling_date=round_date(sampling_date, unit = "day")) %>% 
  # mutate(table_station=sprintf("%s %s",station,bdd_table)) %>% 
  unique()) %>% 
  bind_rows %>%
  group_by(bdd_base)
```

When looking in the timed data for each table, we can conclude that the tables po_comb and Iroise Stanne TEST (IF000997) can be discarded (@fig-global_data)

```{r}
#| label: fig-global_data
#| fig-cap: "Time series of each SNO's tables"
#| cap-location: margin
#| include: true
#| fig-height: 8
#| fig-width: 10

timevision %>% 
  ggplot(aes(x=bdd_table, y=sampling_date, color = bdd_base)) +
  coord_flip() +
  Scalc_lon(4) +
  facet_wrap(~bdd_base, scale="free", ncol=2) +
  geom_jitter(size = 2, alpha = 0.25, width = 0.2) +
  theme(legend.position = "bottom",
        text = element_text(size = 11))

```

```{r}
#| label: reduce_data
# bo_set$bo_hydro<-NULL
po_set$po_comb<-NULL
cf_set$IF000997<-NULL
sno_set<-c(bo_set,po_set,sl_set,cf_set) 
sno_setvar<-c(bo_setvar,po_setvar,sl_setvar,cf_setvar) %>%
  paste(collapse = ", ")
sno_names<-names(sno_set)

timevision <- sno_set %>% lapply(. %>% 
  select(bdd_base,bdd_table,
         sampling_date,station,
         Int_Area_Large, Int_Area_Medium, Int_Area_Small) %>% 
  mutate(sampling_date=round_date(sampling_date, unit = "day")) %>% 
  # mutate(table_station=sprintf("%s %s",station,bdd_table)) %>% 
  unique()) %>% 
  bind_rows %>%
  group_by(bdd_base)
```

## Details on data sets

### Benthobs

```{r}
#| label: benth_data

df_set<-bo_set
df_setvar<-bo_setvar %>%
  paste(collapse = ", ")
df_names<-names(df_set)

name_base <- "BENTHOBS"
df_desc <- sno_all_desc %>% 
  filter(bdd_base %in% name_base) %>% 
  select(-bdd_base) %>% 
  purrr::transpose()

```

The BENTHOBS data set count `r length(df_set)` tables that has the common variables:

> `r df_setvar`

-   **`r df_names[1]`** (n= `r df_desc[[1]]$obsnbtot`) contains `r df_desc[[1]]$varnbtot` variables. Period covered is from `r df_desc[[1]]$yearsrange`, sampling are made mainly at the months of `r df_desc[[1]]$nbstatmt`, on `r df_desc[[1]]$nbstattot` different stations, with a mean of `r df_desc[[1]]$nbstatyr` per year.

-   **`r df_names[2]`** (n= `r df_desc[[2]]$obsnbtot`) contains `r df_desc[[2]]$varnbtot` variables. Period covered is from `r df_desc[[2]]$yearsrange`, sampling are made mainly at the months of `r df_desc[[2]]$nbstatmt`, on `r df_desc[[2]]$nbstattot` different stations, with a mean of `r df_desc[[2]]$nbstatyr` per year.

-   **`r df_names[3]`** (n= `r df_desc[[3]]$obsnbtot`) contains `r df_desc[[3]]$varnbtot` variables. Period covered is from `r df_desc[[3]]$yearsrange`, sampling are made mainly at the months of `r df_desc[[3]]$nbstatmt`, on `r df_desc[[3]]$nbstattot` different stations, with a mean of `r df_desc[[3]]$nbstatyr` per year.

```{r}
#| label: tbl-bo_corr
#| eval: false
#| include: false

# bo_corr_tbl <- bo_set %>% 
#   lapply(. %>% 
#     select(where(is.numeric)) %>% 
#     corstars(., result="none",removeTriangle="upper") %>% 
#     mutate(variable=rownames(.)) %>% 
#     relocate(variable))
# 
# bo_corr_tbl %>% kable()

```

```{r}
#| label: fig-bo_corr
#| eval: false
#| include: false

# bo_corr_fig <- bo_set %>% 
#   lapply(. %>%
#     select(where(is.numeric)) %>% 
#     ggpairs(., #title=titreG,
#       progress=FALSE,
#       upper = list(continuous = corr_col),
#       lower = list(continuous=
#                      wrap("smooth", size = .5,
#                           alpha = 0.4, color = "navyblue"))))
# print(bo_corr_fig)
```

### Phytobs

```{r}
#| label: phyt_data

df_set<-po_set
df_setvar<-po_setvar %>%
  paste(collapse = ", ")
df_names<-names(df_set)

name_base <- "PHYTOBS"
df_desc <- sno_all_desc %>% 
  filter(bdd_base %in% name_base) %>% 
  select(-bdd_base) %>% 
  purrr::transpose()

```

The PHYTOBS data set count `r length(df_set)` tables that has the common variables :

> `r df_setvar`

-   **`r df_names[1]`** (n= `r df_desc[[1]]$obsnbtot`) contains `r df_desc[[1]]$varnbtot` variables. Period covered is from `r df_desc[[1]]$yearsrange`, sampling are made mainly at the months of `r df_desc[[1]]$nbstatmt`, on `r df_desc[[1]]$nbstattot` different stations, with a mean of `r df_desc[[1]]$nbstatyr` per year.

-   **`r df_names[2]`** (n= `r df_desc[[2]]$obsnbtot`) contains `r df_desc[[2]]$varnbtot` variables. Period covered is from `r df_desc[[2]]$yearsrange`, sampling are made mainly at the months of `r df_desc[[2]]$nbstatmt`, on `r df_desc[[2]]$nbstattot` different stations, with a mean of `r df_desc[[2]]$nbstatyr` per year.

-   **`r df_names[3]`** (n= `r df_desc[[3]]$obsnbtot`) contains `r df_desc[[3]]$varnbtot` variables. Period covered is from `r df_desc[[3]]$yearsrange`, sampling are made mainly at the months of `r df_desc[[3]]$nbstatmt`, on `r df_desc[[3]]$nbstattot` different stations, with a mean of `r df_desc[[3]]$nbstatyr` per year.

```{r}
#| label: tbl-po_corr
#| eval: false
#| include: true

# po_corr_tbl <- po_set %>% 
#   lapply(. %>% 
#     select(where(is.numeric)) %>% 
#     corstars(., result="none",removeTriangle="upper") %>% 
#     mutate(variable=rownames(.)) %>% 
#     relocate(variable))
# 
# po_corr_tbl %>% kable()

```

```{r}
#| label: fig-po_corr
#| include: false
#| eval: false

# po_corr_fig <- po_set %>% 
#   lapply(. %>%
#     select(where(is.numeric)) %>% 
#     ggpairs(., #title=titreG,
#       progress=FALSE,
#       upper = list(continuous = corr_col),
#       lower = list(continuous=
#                      wrap("smooth", size = .5,
#                           alpha = 0.4, color = "navyblue"))))
    
# print(po_corr_fig)
```

### Somlit

```{r}
#| label: soml_data

df_set<-sl_set
df_setvar<-sl_setvar %>%
  paste(collapse = ", ")
df_names<-names(df_set)

name_base <- "SOMLIT"
df_desc <- sno_all_desc %>% 
  filter(bdd_base %in% name_base) %>% 
  select(-bdd_base) %>% 
  purrr::transpose()

```

The SOMLIT data set count `r length(df_set)` tables that has the common variables :

> `r df_setvar`

-   **`r df_names[1]`** (n= `r df_desc[[1]]$obsnbtot`) contains `r df_desc[[1]]$varnbtot` variables. Period covered is from `r df_desc[[1]]$yearsrange`, sampling are made mainly at the months of `r df_desc[[1]]$nbstatmt`, on `r df_desc[[1]]$nbstattot` different stations, with a mean of `r df_desc[[1]]$nbstatyr` per year.

-   **`r df_names[2]`** (n= `r df_desc[[2]]$obsnbtot`) contains `r df_desc[[2]]$varnbtot` variables. Period covered is from `r df_desc[[2]]$yearsrange`, sampling are made mainly at the months of `r df_desc[[2]]$nbstatmt`, on `r df_desc[[2]]$nbstattot` different stations, with a mean of `r df_desc[[2]]$nbstatyr` per year.

-   **`r df_names[3]`** (n= `r df_desc[[3]]$obsnbtot`) contains `r df_desc[[3]]$varnbtot` variables. Period covered is from `r df_desc[[3]]$yearsrange`, sampling are made mainly at the months of `r df_desc[[3]]$nbstatmt`, on `r df_desc[[3]]$nbstattot` different stations, with a mean of `r df_desc[[3]]$nbstatyr` per year.

```{r}
#| label: tbl-sl_corr
#| eval: false
#| include: false

# sl_corr_tbl <- sl_set %>% 
#   lapply(. %>% 
#     select(where(is.numeric)) %>% 
#     corstars(., result="none",removeTriangle="upper") %>% 
#     mutate(variable=rownames(.)) %>% 
#     relocate(variable))
# 
# sl_corr_tbl %>% kable()

```

```{r}
#| label: fig-sl_corr
#| include: false
#| eval: false

# sl_corr_fig <- sl_set %>% 
#   lapply(. %>%
#     select(where(is.numeric)) %>% 
#     ggpairs(., #title=titreG,
#       progress=FALSE,
#       upper = list(continuous = corr_col),
#       lower = list(continuous=
#                      wrap("smooth", size = .5,
#                           alpha = 0.4, color = "navyblue"))))
# print(sl_corr_fig)
```

### Coasthf

```{r}
#| label: coas_data

df_set<-cf_set
df_setvar<-cf_setvar %>%
  paste(collapse = ", ")
df_names<-names(df_set)

name_base <- "COASTHF"
df_desc <- sno_all_desc %>% 
  filter(bdd_base %in% name_base) %>% 
  select(-bdd_base) %>% 
  purrr::transpose()

```

The COASTHF data set count `r length(df_set)` tables that has the common variables :

> `r df_setvar`

```{r}
#| include: true
bind_rows(df_desc, .id = "bdd_table")  %>%
  kable()
```

```{r}
#| label: tbl-cf_corr
#| eval: false
#| include: false

# cf_corr_tbl <- cf_set %>% 
#   lapply(. %>% 
#     select(where(is.numeric)) %>% 
#     corstars(., result="none",removeTriangle="upper") %>% 
#     mutate(variable=rownames(.)) %>% 
#     relocate(variable))
# 
# cf_corr_tbl %>% kable()

```

```{r}
#| label: fig-cf_corr
#| include: false
#| eval: false

# cf_corr_fig <- cf_set %>% 
#   lapply(. %>%
#     select(where(is.numeric)) %>% 
#     ggpairs(., #title=titreG,
#       progress=FALSE,
#       upper = list(continuous = corr_col),
#       lower = list(continuous=
#                      wrap("smooth", size = .5,
#                           alpha = 0.4, color = "navyblue"))))
# print(cf_corr_fig)
```

Data can then be distinguished by geographical areas, with the different scale chosen: first the large one (@fig-large_data) that emphasize that the Channel, the Atlantic and north Brittany are of the most interest. The small scale (@fig-small_data) is the preferred scale reveal the challenge of the workshop. The intermediate scale (@fig-medium_data) shows that when there is geographically close SNO data sets, they can be temporarily not that relevant.

```{r}
#| label: fig-large_data
#| fig-cap: "Time series of SNOs tables by large area"
#| cap-location: margin
#| include: true
#| column: page
#| fig-height: 10
#| fig-width: 15

gp <- timevision %>% 
  ggplot(aes(x=station, y=sampling_date, color=bdd_base)) +
  coord_flip() +
  Scalc_lon(4) +
  facet_wrap(~Int_Area_Large, scale="free", ncol=2) +
  geom_jitter(size = 2, alpha = 0.4, width = 0.2) +
  theme(legend.position = "bottom",
        text = element_text(size = 10));gp

```

## Small area time series

::: panel-tabset
```{r}
#| label: prefig-small_data

large_areas<-SNO_Areas %>% 
  filter(Category=="LARGE") %>% 
  filter(!Int_Area=="Sud Bretagne") %>% 
  distinct(Int_Area) %>% 
  pull %>% 
  as.list

zoom_small<-function(timevision,IntAreaLarge) { 
    timevision %>% 
    filter(Int_Area_Large==IntAreaLarge) %>% 
    drop_na(Int_Area_Small) %>% 
  ggplot(aes(x=station, y=sampling_date, color=bdd_base)) +
  coord_flip() +
  Scalc_lon(4) +
  facet_wrap(~Int_Area_Small, scale="free", ncol=2) +
  geom_jitter(size = 2, alpha = 0.4, width = 0.2) +
  theme(legend.position = "bottom",
        text = element_text(size = 10))
  ggplotly(gp)
  }
zoom_small_plots<-map(large_areas,~zoom_small(timevision,.))

map_small <- function(tm_data,SNO_Areas,IntAreaLarge){
  tm_data +
    tm_view(bbox = st_bbox(SNO_Areas %>% filter(Int_Area==IntAreaLarge)))
  }
zoom_small_map<-map(large_areas,~map_small(tm_data,SNO_Areas,.))

```

### Manche

```{r}
#| label: fig-small_data
#| fig-cap: "Time series of SNOs tables by small area"
#| cap-location: margin
#| include: true
#| column: screen-inset-shaded
#| fig-format: svg
#| fig-height: 5
#| layout-ncol: 2

zoom_small_plots[[1]]
zoom_small_map[[1]]
```

### Nord Bretagne

```{r}
#| label: fig-small_data2
#| fig-cap: "Time series of SNOs tables by small area"
#| cap-location: margin
#| include: true
#| column: screen-inset-shaded
#| fig-format: svg
#| fig-height: 5
#| layout-ncol: 2

zoom_small_plots[[2]]
zoom_small_map[[2]]
```

### Atlantique

```{r}
#| label: fig-small_data3
#| fig-cap: "Time series of SNOs tables by small area"
#| cap-location: margin
#| include: true
#| column: screen-inset-shaded
#| fig-format: svg
#| fig-height: 5
#| layout-ncol: 2

zoom_small_plots[[3]]
zoom_small_map[[3]]
```

### Mediterranee

```{r}
#| label: fig-small_data4
#| fig-cap: "Time series of SNOs tables by small area"
#| cap-location: margin
#| include: true
#| column: screen-inset-shaded
#| fig-format: svg
#| fig-height: 5
#| layout-ncol: 2

zoom_small_plots[[4]]
zoom_small_map[[4]]
```
:::

## Medium area time series

::: panel-tabset
```{r}
#| label: prefig-medi_data

large_areas<-SNO_Areas %>% 
  filter(Category=="LARGE") %>% 
  filter(!Int_Area=="Sud Bretagne") %>% 
  distinct(Int_Area) %>% 
  pull %>% 
  as.list

zoom_medi<-function(timevision,IntAreaLarge) { 
    timevision %>% 
    filter(Int_Area_Large==IntAreaLarge) %>% 
    drop_na(Int_Area_Medium) %>% 
  ggplot(aes(x=station, y=sampling_date, color=bdd_base)) +
  coord_flip() +
  Scalc_lon(4) +
  facet_wrap(~Int_Area_Medium, scale="free", ncol=2) +
  geom_jitter(size = 2, alpha = 0.4, width = 0.2) +
  theme(legend.position = "bottom",
        text = element_text(size = 10))
  ggplotly(gp)
  }
zoom_medi_plots<-map(large_areas,~zoom_medi(timevision,.))

map_medi <- function(tm_data,SNO_Areas,IntAreaLarge){
  tm_data +
    tm_view(bbox = st_bbox(SNO_Areas %>% filter(Int_Area==IntAreaLarge)))
  }
zoom_medi_map<-map(large_areas,~map_medi(tm_data,SNO_Areas,.))

```

### Manche

```{r}
#| label: fig-medium_data
#| fig-cap: "Time series of SNOs tables by medium area"
#| cap-location: margin
#| include: true
#| column: screen-inset-shaded
#| fig-format: svg
#| fig-height: 5
#| layout-ncol: 2

zoom_medi_plots[[1]]
zoom_medi_map[[1]]
```

### Nord Bretagne

```{r}
#| label: fig-medium_data2
#| fig-cap: "Time series of SNOs tables by small area"
#| cap-location: margin
#| include: true
#| column: screen-inset-shaded
#| fig-format: svg
#| fig-height: 5
#| layout-ncol: 2

zoom_medi_plots[[2]]
zoom_medi_map[[2]]
```

### Atlantique

```{r}
#| label: fig-medium_data3
#| fig-cap: "Time series of SNOs tables by small area"
#| cap-location: margin
#| include: true
#| column: screen-inset-shaded
#| fig-format: svg
#| fig-height: 5
#| layout-ncol: 2

zoom_medi_plots[[3]]
zoom_medi_map[[3]]
```

### Mediterranee

```{r}
#| label: fig-medium_data4
#| fig-cap: "Time series of SNOs tables by small area"
#| cap-location: margin
#| include: true
#| column: screen-inset-shaded
#| fig-format: svg
#| fig-height: 5
#| layout-ncol: 2

zoom_medi_plots[[4]]
zoom_medi_map[[4]]
```
:::

# Final actions and save

Rdata are saved in `rdataout`. An excel file collects data in `r binderout` , with sheets for :

-   data : contains whatever

```{r}
#| label: finalsave_xsl
#| eval: false

# wb <- copyWorkbook(loadWorkbook(binderin))
# # copyworkbook to avoid any pb that can occur on the excel file
# if (!("AllMeas" %in% names(wb))) {
#   addWorksheet(wb=wb, sheetName = "AllMeas")}
# writeData(wb, sheet = "AllMeas", x = data, 
#           startCol = 1, startRow = 1,withFilter = FALSE)
# 
# saveWorkbook(wb,file=binderout, overwrite = TRUE)

```

```{r}
#| label: finalsave_rdata
rm(list=ls(pattern="raw"))
rm(list=ls(pattern="tmp"))
rm(list=ls(pattern="corr"))
rm(sno_set,df_set,df,df_desc)
# save.image(file = rdataout)
beepr::beep(2)

```

# References {.unnumbered}
